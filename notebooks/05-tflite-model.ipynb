{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d72c28",
   "metadata": {},
   "source": [
    "# TensorFlow Lite model for deployment\n",
    "\n",
    "The TFLite model can be deployed on cloud platforms as a web service via REST API, allowing for larger inputs and more complex models to be processed, it also enables scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a848333b",
   "metadata": {},
   "source": [
    "## Convert Keras model to TFLite\n",
    "\n",
    "We have created a model using Keras and now we are going to convert it into TensorFlow Lite (TFLite) format to use it in a web application. This conversion process involves several steps to adapt the model to this specific format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f573dab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\awon\\AppData\\Local\\Temp\\tmpgdvfccd3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\awon\\AppData\\Local\\Temp\\tmpgdvfccd3\\assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('../models/xception_v5_01_0.985.h5')\n",
    "\n",
    "# Initialize tf-lite converter for keras model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# Make model conversion\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model in tflite format\n",
    "with open('../models/wildfire-model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddf3c7f",
   "metadata": {},
   "source": [
    "## Removing TensorFlow dependencies\n",
    "\n",
    "We want to reduce the size of our TensorFlow model for deployment by removing unnecessary TensorFlow dependencies using TensorFlow Model Optimization Toolkit or TensorFlow Lite's tflite_convert tool, which can prune, quantize and compress the model, resulting in a smaller and more efficient model.\n",
    "\n",
    "A simpler way of loading and preprocessing image is using [keras-image-helper](https://pypi.org/project/keras-image-helper/), also, our model needs tf-lite from tensorflow package, however, we can use tf-lite without depending on on tensorflow and this is what we are looking for. For this, we need to install `tflite-runtime` from GitHub Coral Python page.\n",
    "\n",
    "A more straightforward method for loading and preparing images is by utilizing the [keras-image-helper](https://pypi.org/project/keras-image-helper/) package. Additionally, our model requires the `tf-lite` module from the Tensorflow library. However, it is possible to use `tf-lite` without depending on the Tensorflow package, which is what we are aiming for. To achieve this, we must install the tflite-runtime from the [GitHub Coral Python](https://github.com/google-coral/py-repo) page. `tflite-runtime` can be installed using `pip install --extra-index-url https://google-coral.github.io/py-repo/ tflite-runtime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc73f5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflite_runtime.interpreter as tflite\n",
    "from keras_image_helper import create_preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c098cd8",
   "metadata": {},
   "source": [
    "## Make predictions with tflite model\n",
    "\n",
    "To make predictions using TensorFlow Lite (TFLite), you must first load a pre-trained TFLite model using the TFLite Interpreter API. Then, you can use the interpreter to run inference on your input data, which should match the shape and type expected by the model. The interpreter will return the output in the form of a multi-dimensional array, containing the model's predictions for the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af076315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model using Interpreter class\n",
    "interpreter = tflite.Interpreter(model_path='../models/wildfire-model.tflite')\n",
    "# Load the weights from the model to memory\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get the input index from interpreter\n",
    "input_index = interpreter.get_input_details()[0]['index']\n",
    "# Get the output index from interpreter\n",
    "output_index = interpreter.get_output_details()[0]['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0997cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessor for the model we used for training (Xception)\n",
    "preprocessor = create_preprocessor('xception', target_size=(299,299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51b7516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the image (from path)\n",
    "X_path = preprocessor.from_path('../assets/land_img.jpg')\n",
    "\n",
    "# Preprocess the image (from url)\n",
    "X_url = preprocessor.from_url('https://ichef.bbci.co.uk/news/976/cpsprodpb/32A5/production/_123456921_australianbushfire_gettyimages-1198540877.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99b39933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fire': 0.0, 'nofire': 1.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make prediction on image from path\n",
    "interpreter.set_tensor(input_index, X_path)\n",
    "interpreter.invoke()\n",
    "preds = interpreter.get_tensor(output_index)[0]\n",
    "\n",
    "# List of classes the model was trained to predict\n",
    "classes = ['fire', 'nofire']\n",
    "\n",
    "# Check the prediction scores\n",
    "dict(zip(classes, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e4ce50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fire': 1.0, 'nofire': 0.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make prediction on image from url ()\n",
    "interpreter.set_tensor(input_index, X_url)\n",
    "interpreter.invoke()\n",
    "preds = interpreter.get_tensor(output_index)[0]\n",
    "\n",
    "# Check the prediction scores\n",
    "dict(zip(classes, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
